{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook only has augmented data and simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random \n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SCALER = StandardScaler()\n",
    "\n",
    "def print_data_summary(data):\n",
    "    print(\"There are {0} datapoints\".format(len(data)))\n",
    "    print(\"There are {0} columns\".format(len(data.columns)))\n",
    "    try:\n",
    "        print(\"There are {0} diffenret movies\".format(len(set(data['s_video_id']))))\n",
    "    except:\n",
    "        pass\n",
    "    # print(data.describe())\n",
    "    \n",
    "def encode_one_hot(data):\n",
    "    categorical_columns = get_categorical_columns(data)\n",
    "    oneHotEncodedData = pd.get_dummies(data, columns=categorical_columns)\n",
    "    \n",
    "    # Test\n",
    "    newNumberOfColumns = len(oneHotEncodedData.columns) \n",
    "\n",
    "    totalDifferentValues = 0\n",
    "    for c in categorical_columns:\n",
    "        totalDifferentValues += len(data[c].unique())\n",
    "\n",
    "    # One hot encoding removes each categorical columns (- len(nonNumericalList)) \n",
    "    # and adds n columns where n is number of different values taken by column (+ totalDifferentValues)\n",
    "    assert len(data.columns) + totalDifferentValues - len(categorical_columns) == newNumberOfColumns\n",
    "    \n",
    "    return oneHotEncodedData\n",
    "\n",
    "def split_train_test(data, test_mask):\n",
    "    # split between train and test\n",
    "    train_mask = list(map(lambda b: not b, test_mask))\n",
    "\n",
    "    train = data[train_mask]\n",
    "    test = data[test_mask]\n",
    "\n",
    "    assert len(train) + len(test) == len(data) \n",
    "\n",
    "    y_train = train.loc[:, ['t_average_vmaf']]\n",
    "    x_train = train.drop(['t_average_vmaf'], axis=1)\n",
    "    assert type(x_train) == type(y_train) \n",
    "\n",
    "    y_test = test.loc[:, ['t_average_vmaf']]\n",
    "    x_test = test.drop(['t_average_vmaf'], axis=1)\n",
    "    assert type(x_test) == type(y_test)\n",
    "\n",
    "    assert len(x_train) == len(y_train)\n",
    "    assert len(x_train.columns) + len(y_train.columns) == len(data.columns)\n",
    "    assert len(x_test) == len(y_test)\n",
    "    assert len(x_test.columns) + len(y_test.columns) == len(data.columns)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def calculate_rmse(y_true, y_predictions):\n",
    "    return mean_squared_error(y_true, y_predictions, squared=False)\n",
    "\n",
    "def calculate_std(y_true, y_predictions):\n",
    "    '''\n",
    "    Standard deviation is calculated considering the absoulte values of the residuals as the\n",
    "    values and the RMSE as the mean.\n",
    "    '''\n",
    "    rmse = calculate_rmse(y_true, y_predictions)\n",
    "    \n",
    "    abs_residuals = np.absolute(y_true - y_predictions)\n",
    "    \n",
    "    \n",
    "    #print(abs_residuals)\n",
    "    #print(abs_residuals.tolist().sort(reverse=True))\n",
    "    tbp = list(np.ravel(abs_residuals))\n",
    "    tbp.sort(reverse=True)\n",
    "    print(\"Printing the 4th quartile to doublecheck std: \")\n",
    "    print(np.percentile(tbp, 75))  # Q3\n",
    "    #print(tbp[0:20])\n",
    "\n",
    "    \n",
    "    rmse_column = np.array([rmse]*len(abs_residuals)).reshape(len(abs_residuals), 1)\n",
    "\n",
    "    #print(\"len(abs_residuals)\")\n",
    "    #print(len(abs_residuals))\n",
    "    #print(\"rmse_column\")\n",
    "    #print(rmse_column)\n",
    "    variance = np.sum(np.square(abs_residuals - rmse_column)) / len(abs_residuals)\n",
    "    std = math.sqrt(variance)\n",
    "    return std\n",
    "\n",
    "def run_model(regression_model, x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Given a sklearn model and the train / test data,\n",
    "    returns the rmse, std and r squared scores of the model\n",
    "    also returns training time in seconds\n",
    "    '''\n",
    "    start_time = datetime.now().timestamp()\n",
    "   \n",
    "    regular_regression = regression_model.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "    end_time = datetime.now().timestamp()\n",
    "   \n",
    "    predictions = regression_model.predict(x_test.to_numpy())\n",
    "\n",
    "    rmse = calculate_rmse(y_test.to_numpy(), predictions)\n",
    "\n",
    "    std = calculate_std(y_test.to_numpy(), predictions)\n",
    "\n",
    "    coefficient_of_determination = regression_model.score(x_test.to_numpy(), y_test.to_numpy())\n",
    "    \n",
    "    return Model_results(rmse, std, coefficient_of_determination, end_time - start_time, regression_model)\n",
    "\n",
    "def analyse_model(regression_model, x_train, y_train, x_test, y_test):\n",
    "    regular_regression = regression_model.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "    \n",
    "    paramRelevance = {'Parameter':x_train.columns.to_list()[:],\n",
    "            'Weight':regular_regression.coef_.tolist()[0]}\n",
    " \n",
    "    dfParam = pd.DataFrame(paramRelevance)\n",
    "\n",
    "    params = dfParam.reindex(dfParam.Weight.abs().sort_values(ascending=False).index)\n",
    "    #print(\"there are \" + str(len(params.iloc[:, 1])) + \" params\")\n",
    "    #print(params)\n",
    "    print(params.iloc[0:40, :])\n",
    "    print(params.iloc[40:80, :])\n",
    "    print(params.iloc[80:120, :])\n",
    "    print(params.iloc[120:160, :])\n",
    "    print(params.iloc[160:200, :])\n",
    "    print(params.iloc[200:240, :])\n",
    "    print(params.iloc[240:280, :])\n",
    "    print(params.iloc[320:360, :])\n",
    "    print(params.iloc[360:400, :])\n",
    "    print(params.iloc[400:450, :])\n",
    "    print(params.iloc[450:500, :])\n",
    "    print(params.iloc[500:550, :])\n",
    "    print(params.iloc[550:600, :])\n",
    "    print(params.iloc[600::, :])\n",
    "\n",
    "class Model_results:\n",
    "    def __init__(self, rmse, std, cod, time, model):\n",
    "        self.rmse = rmse\n",
    "        self.std = std\n",
    "        self.cod = cod\n",
    "        self.time = time\n",
    "        self.model = model\n",
    "        \n",
    "    def __str__(self):\n",
    "        attrs = vars(self)\n",
    "        return ', '.join(\"%s: %s\" % item for item in attrs.items())\n",
    "    \n",
    "def get_numerical_columns(data):\n",
    "    numerical = data.select_dtypes(exclude=['object'])\n",
    "    exclude_categorical = [col for col in numerical.columns \\\n",
    "                                   if not 'content_category' in col \\\n",
    "                                   and not 'scan_type' in col \\\n",
    "                                   and not 'codec_profile' in col \\\n",
    "                                   and not 's_video_id' in col \\\n",
    "                                    and not 's_size' in col]\n",
    "    return exclude_categorical\n",
    "\n",
    "def get_categorical_columns(data):\n",
    "    numerical_columns = set(get_numerical_columns(data))\n",
    "    return list(set(data.columns) - numerical_columns)\n",
    "\n",
    "def explore_categorical_features(data):\n",
    "    # explorig categorical columns\n",
    "    nonNumericalFrame = data.select_dtypes(include=['object']).copy()\n",
    "    nonNumericalList = nonNumericalFrame.columns.tolist()\n",
    "    totalDifferentValues = 0\n",
    "    for c in nonNumericalList:\n",
    "        print(\"{0} is non-numerical and values are: {1}\".format(c, nonNumericalFrame[c].unique()))\n",
    "        totalDifferentValues += len(nonNumericalFrame[c].unique())\n",
    "\n",
    "    print(\"There are {0} total possible values among the categorical columns\".format(totalDifferentValues))\n",
    "\n",
    "    # content_category is the most problematic column because it has many possible values\n",
    "    # nonNumericalFrame[\"c_content_category\"].value_counts()\n",
    "    \n",
    "def getColumnsWithNa(data):\n",
    "    nan_values = data.isna()\n",
    "    nan_columns = nan_values.any()\n",
    "    columns_with_nan = data.columns[nan_columns].tolist()\n",
    "    return columns_with_nan\n",
    "\n",
    "def augmentData(data, targetColumnsName):\n",
    "    y = data[targetColumnsName]\n",
    "    exclude_categorical = get_numerical_columns(data)\n",
    "\n",
    "    baseForAugmentation = data[exclude_categorical]\n",
    "    baseForAugmentation = baseForAugmentation.drop([targetColumnsName], axis = 1)\n",
    "    columnsCopy = baseForAugmentation.columns.tolist()\n",
    "\n",
    "    print(\"original columns {0}\".format(len(baseForAugmentation.columns)))\n",
    "\n",
    "    for c in baseForAugmentation.columns:\n",
    "\n",
    "        data[c + \"_squared\"] = data[c] ** 2\n",
    "        data[c + \"_cubic\"] = data[c] ** 3\n",
    "\n",
    "        columnsCopy.remove(c)\n",
    "        for cc in columnsCopy:\n",
    "            t = data[c] * data[cc]\n",
    "            data[c + \"_*_\" + cc ] = data[c] * data[cc]\n",
    "    return data\n",
    "\n",
    "def scaleData(data, targetColumnName, fitScaler=False):\n",
    "    # split data in numerical, categorical, y columns\n",
    "    categoricalColumns = get_categorical_columns(data.drop([targetColumnName], axis = 1))\n",
    "    \n",
    "    categorical = data[categoricalColumns]\n",
    "    numerical = data[get_numerical_columns(data.drop([targetColumnName], axis = 1))]\n",
    "\n",
    "    assert len(categorical.columns) + len(numerical.columns) + 1 == len(data.columns)\n",
    "\n",
    "    # scale only numerical\n",
    "    global SCALER\n",
    "    if(fitScaler):\n",
    "        SCALER.fit(numerical)\n",
    "    scaled_numerical = SCALER.transform(numerical)\n",
    "\n",
    "    # recombine everything\n",
    "    numerical_data = pd.DataFrame(scaled_numerical, numerical.index, columns=numerical.columns)\n",
    "    # test\n",
    "    assert len(numerical_data.columns) + len(categorical.columns) + 1 == len(augmented_data.columns)\n",
    "    data = pd.concat([numerical_data, categorical, data[targetColumnName]], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "rawColumns = data.columns\n",
    "rawData = deepcopy(data)\n",
    "\n",
    "# test \n",
    "assert len(get_numerical_columns(data)) + len(get_categorical_columns(data)) == len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns have been removed because empty: {'t_average_vmaf_4k', 't_average_psnr', 't_average_vmaf_mobile'}\n",
      "The rows with missing vmaf (target) are: 396\n",
      "There were 2883 rows deleted because they had less than 41 valorised features\n",
      "\n",
      "After 1 hot encoding categorical features\n",
      "There are 12133 datapoints\n",
      "There are 44 columns\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# Remove empty columns\n",
    "data = data.dropna(axis=1, thresh=len(data.columns))\n",
    "print(\"The following columns have been removed because empty: {0}\".format(set(rawColumns) - set(data.columns)))\n",
    "\n",
    "# Remove video id\n",
    "data = data.drop(['s_video_id'], axis = 1)\n",
    "\n",
    "# Coalesce width and height to a categorical feature\n",
    "# print(data.groupby(['s_width','s_height']).size().reset_index().rename(columns={0:'count'}))\n",
    "\n",
    "data['s_size'] = data.apply(lambda row: str(row['s_width']) + \"_\" + str(row['s_height']), axis=1)\n",
    "data = data.drop(['s_width', 's_height'], axis = 1)\n",
    "\n",
    "\n",
    "######### BEGIN Experimantal excurusus \n",
    "\n",
    "# Deal with rows with missing values\n",
    "threshold = len(data.columns)\n",
    "\n",
    "# get rows with missing values\n",
    "na_free = data.dropna(axis=0, thresh=threshold)\n",
    "only_na = data[~data.index.isin(na_free.index)]\n",
    "\n",
    "missing_vmaf = data[data['t_average_vmaf'].isnull()]\n",
    "print(\"The rows with missing vmaf (target) are: \" + str(len(missing_vmaf)))\n",
    "\n",
    "na_row_without_na_vmaf = only_na[~only_na.index.isin(missing_vmaf.index)]\n",
    "assert len(na_row_without_na_vmaf) + len(missing_vmaf) == len(only_na)\n",
    "\n",
    "# now I got all the rows with NA except those with NA in the vmaf\n",
    "\n",
    "# drop some columns because they always had same value \n",
    "columnsWithAlwaysTheSameValue_AfterRemovingALLNanRows = ['e_aspect_ratio', 'e_pixel_aspect_ratio', 'e_codec', 'e_b_frame_int', 'e_ref_frame_count', 'e_bit_depth', 'e_pixel_fmt']\n",
    "na_row_without_na_vmaf = na_row_without_na_vmaf.drop(columnsWithAlwaysTheSameValue_AfterRemovingALLNanRows, axis=1)\n",
    "removeBecauseRemovedInNormalPipelineAswell = ['c_content_category']\n",
    "na_row_without_na_vmaf = na_row_without_na_vmaf.drop(removeBecauseRemovedInNormalPipelineAswell, axis=1)\n",
    "\n",
    "# those columns have not been removed and they are categorical\n",
    "todealwith = ['e_codec_profile', 's_scan_type', 'e_scan_type']\n",
    "na_row_without_na_vmaf[todealwith] = na_row_without_na_vmaf[todealwith].fillna(value='fillerValue')\n",
    "\n",
    "na_row_without_na_vmaf = na_row_without_na_vmaf.fillna(value=0)\n",
    "\n",
    "assert len(getColumnsWithNa(na_row_without_na_vmaf)) == 0\n",
    "\n",
    "na_row_without_na_vmaf = encode_one_hot(na_row_without_na_vmaf)\n",
    "\n",
    "# remove the filler columns\n",
    "for c in na_row_without_na_vmaf.columns:\n",
    "    if \"filler\" in c:\n",
    "        na_row_without_na_vmaf = na_row_without_na_vmaf.drop([c], axis=1)\n",
    "\n",
    "#########   END Experimantal excurusus      \n",
    "\n",
    "# Remove rows with at least 1 nan\n",
    "data = data.dropna(axis=0, thresh=threshold)\n",
    "print(\"There were {0} rows deleted because they had less than {1} valorised features\".format(len(rawData) - len(data), threshold))\n",
    "\n",
    "# Remove columns that always have the same value (has to be after nan removal)\n",
    "nunique = data.apply(pd.Series.nunique)\n",
    "cols_to_drop = nunique[nunique == 1].index\n",
    "data = data.drop(cols_to_drop, axis=1)\n",
    "for c in cols_to_drop.tolist():\n",
    "    #print(\"Column {0} always has same value: {1}, removed\".format(c, rawData[c].tolist()[1]))\n",
    "    pass\n",
    "assert len(nunique) == len(data.columns) + len(cols_to_drop)\n",
    "\n",
    "\n",
    "#print(\"\\nAfter inital processing\")\n",
    "#print_data_summary(data)\n",
    "\n",
    "#explore_categorical_features(data)\n",
    "\n",
    "# REMOVE CATEGORY\n",
    "data = data.drop(['c_content_category'], axis = 1)\n",
    "\n",
    "# One hot encoding categorical columns\n",
    "\n",
    "data = encode_one_hot(data)\n",
    "print(\"\\nAfter 1 hot encoding categorical features\")\n",
    "print_data_summary(data)\n",
    "\n",
    "preprocessed_data = deepcopy(data)\n",
    "\n",
    "# because of one hot encoding there are specific columns (ie specific to a value) that are not present in na_row_without_na_vmaf\n",
    "# TODO INET IN RIGHT PLACE \n",
    "columnsRelatedToSpecificScreenSizes = list(set(preprocessed_data.columns) - set(na_row_without_na_vmaf.columns))\n",
    "for s in columnsRelatedToSpecificScreenSizes:\n",
    "    na_row_without_na_vmaf[s] = 0\n",
    "    \n",
    "assert len(list(set(preprocessed_data.columns) - set(na_row_without_na_vmaf.columns))) == 0\n",
    "assert len(preprocessed_data.columns) == len(na_row_without_na_vmaf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After augmenting the data\n",
      "There are 12133 datapoints\n",
      "There are 44 columns\n",
      "There are 2487 datapoints\n",
      "There are 44 columns\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "\n",
    "# from [x, y, z] we want to obtain \n",
    "# [x, y, z, x**2, y**2, z**2, x*y, x*z, y*z]\n",
    "# aka all the degree 2 polinomyals \n",
    "\n",
    "# only consider numerical columns, exluce categorical\n",
    "\n",
    "#preprocessed_data = augmentData(preprocessed_data, 't_average_vmaf')\n",
    "#na_row_without_na_vmaf = augmentData(na_row_without_na_vmaf, 't_average_vmaf')\n",
    "\n",
    "print(\"After augmenting the data\")\n",
    "print_data_summary(preprocessed_data)\n",
    "print_data_summary(na_row_without_na_vmaf)\n",
    "augmented_data = deepcopy(preprocessed_data)\n",
    "augmented_data.head()\n",
    "assert len(augmented_data['t_average_vmaf']) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After scaling\n",
      "There are 12133 datapoints\n",
      "There are 44 columns\n",
      "\n",
      "After scaling na_row_without_na_vmaf\n",
      "There are 2487 datapoints\n",
      "There are 44 columns\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True]\n",
      "set()\n",
      "Index(['e_codec_profile_main', 'e_scan_type_not supported yet',\n",
      "       's_size_1920_800', 's_size_720_608', 's_size_352_288',\n",
      "       's_scan_type_progressive', 'e_scan_type_interlaced'],\n",
      "      dtype='object')\n",
      "Index(['e_codec_profile_main', 'e_scan_type_not supported yet',\n",
      "       's_size_1920_800', 's_size_720_608', 's_size_352_288',\n",
      "       's_scan_type_progressive', 'e_scan_type_interlaced'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 6 Scale data (mean is zero)\n",
    "\n",
    "data = scaleData(augmented_data, 't_average_vmaf', fitScaler=True)\n",
    "# data = augmented_data\n",
    "na_row_without_na_vmaf = scaleData(na_row_without_na_vmaf, 't_average_vmaf')\n",
    "#na_row_without_na_vmaf = na_row_without_na_vmaf\n",
    "\n",
    "\n",
    "print(\"\\nAfter scaling\")\n",
    "print_data_summary(data)\n",
    "print(\"\\nAfter scaling na_row_without_na_vmaf\")\n",
    "print_data_summary(na_row_without_na_vmaf)\n",
    "print(data.columns == na_row_without_na_vmaf.columns)\n",
    "\n",
    "print(set(data.columns) - set(na_row_without_na_vmaf.columns))\n",
    "\n",
    "print(data.columns[34:41])\n",
    "print(na_row_without_na_vmaf.columns[34:41])\n",
    "\n",
    "assert len(data.columns) == len(na_row_without_na_vmaf.columns)\n",
    "assert (data.columns == na_row_without_na_vmaf.columns).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitthe 3 datasets (preprocessed, augmented, PCAed)\n",
    "# Note the the same splitting is applied across all 3 \n",
    "\n",
    "test_mask = np.random.rand(len(data), 1) > 0.40\n",
    "\n",
    "x_train_aug, y_train_aug, x_test_aug, y_test_aug = split_train_test(augmented_data, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular regression\n",
      "51.301607086796565\n",
      "7.576983687492098\n"
     ]
    }
   ],
   "source": [
    "# Regressions\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "\n",
    "# analyse_model(LinearRegression(), x_train_aug, y_train_aug, x_test_aug, y_test_aug)\n",
    "\n",
    "# regular\n",
    "print(\"Regular regression\")\n",
    "LR = LinearRegression()\n",
    "LR = LR.fit(x_train_aug.to_numpy(), y_train_aug.to_numpy())\n",
    "y = na_row_without_na_vmaf['t_average_vmaf']\n",
    "na_row_without_na_vmaf = na_row_without_na_vmaf.drop(['t_average_vmaf'], axis=1)\n",
    "\n",
    "print(calculate_rmse(y, LR.predict(na_row_without_na_vmaf.to_numpy())))\n",
    "print(calculate_rmse(y_test_aug, LR.predict(x_test_aug.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(x_test_aug), 13):\n",
    "    print(LR.predict([x_test_aug.to_numpy()[i]])[0:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91.3159151]]\n",
      "[[90.9291089]]\n",
      "[[96.31350262]]\n",
      "[[92.03169969]]\n",
      "[[95.82798982]]\n",
      "[[89.82922363]]\n",
      "[[99.95027155]]\n",
      "[[88.02832055]]\n",
      "[[92.8326334]]\n",
      "[[87.58395713]]\n",
      "[[90.80010904]]\n",
      "[[84.22122104]]\n",
      "[[67.82396815]]\n",
      "[[56.48201641]]\n",
      "[[62.25320384]]\n",
      "[[56.09523868]]\n",
      "[[55.76892509]]\n",
      "[[65.88997208]]\n",
      "[[53.96802444]]\n",
      "[[58.77233487]]\n",
      "[[53.52365746]]\n",
      "[[56.73980922]]\n",
      "[[55.7729131]]\n",
      "[[68.55163202]]\n",
      "[[68.38405846]]\n",
      "[[78.95241489]]\n",
      "[[73.7684443]]\n",
      "[[73.38167058]]\n",
      "[[72.89616066]]\n",
      "[[72.5093968]]\n",
      "[[76.43828714]]\n",
      "[[64.51635026]]\n",
      "[[65.03888549]]\n",
      "[[68.25503725]]\n",
      "[[67.28814108]]\n",
      "[[56.49255682]]\n",
      "[[66.61393808]]\n",
      "[[57.14799578]]\n",
      "[[56.76122361]]\n",
      "[[60.55751433]]\n",
      "[[54.5587309]]\n",
      "[[58.91580615]]\n",
      "[[57.94890042]]\n",
      "[[56.88326773]]\n",
      "[[51.73332045]]\n",
      "[[54.94947321]]\n",
      "[[25.61199277]]\n",
      "[[35.73312538]]\n",
      "[[35.73312528]]\n",
      "[[25.88065841]]\n",
      "[[24.06494739]]\n",
      "[[29.29018661]]\n",
      "[[28.90342568]]\n",
      "[[21.29714329]]\n",
      "[[26.10145936]]\n",
      "[[25.134562]]\n",
      "[[19.88588796]]\n",
      "[[64.02114094]]\n",
      "[[58.02235207]]\n",
      "[[57.63559029]]\n",
      "[[57.18837651]]\n",
      "[[58.29101836]]\n",
      "[[57.90425717]]\n",
      "[[57.51749673]]\n",
      "[[55.12165221]]\n",
      "[[53.12736611]]\n",
      "[[53.6499059]]\n",
      "[[62.14858595]]\n",
      "[[42.39812409]]\n",
      "[[46.19441367]]\n",
      "[[40.13519745]]\n",
      "[[50.70350452]]\n",
      "[[50.3167159]]\n",
      "[[40.464305]]\n",
      "[[40.07754446]]\n",
      "[[43.29369474]]\n",
      "[[36.71480237]]\n",
      "[[45.67552906]]\n",
      "[[33.75361832]]\n",
      "[[13.52963169]]\n",
      "[[13.017845]]\n",
      "[[11.19161348]]\n",
      "[[12.62056688]]\n",
      "[[17.14011386]]\n",
      "[[11.59578863]]\n",
      "[[11.18275891]]\n",
      "[[10.1171236]]\n",
      "[[7.71077159]]\n",
      "[[8.17282045]]\n",
      "[[11.53206838]]\n",
      "[[82.53866243]]\n",
      "[[82.05316845]]\n",
      "[[80.22695604]]\n",
      "[[80.22695604]]\n",
      "[[79.84019644]]\n",
      "[[85.78866166]]\n",
      "[[80.24435907]]\n",
      "[[79.2512004]]\n",
      "[[78.28430329]]\n",
      "[[75.77922907]]\n",
      "[[80.56741159]]\n",
      "[[116.14460793]]\n",
      "[[114.71558108]]\n",
      "[[119.65286021]]\n",
      "[[115.37101147]]\n",
      "[[113.55526485]]\n",
      "[[112.72130771]]\n",
      "[[118.3937462]]\n",
      "[[111.36759915]]\n",
      "[[116.07317742]]\n",
      "[[115.20500979]]\n",
      "[[113.27121714]]\n",
      "[[113.17248811]]\n",
      "[[45.50940257]]\n",
      "[[34.16733286]]\n",
      "[[39.55174629]]\n",
      "[[39.06624116]]\n",
      "[[33.06747784]]\n",
      "[[42.02821825]]\n",
      "[[30.10629349]]\n",
      "[[34.91061031]]\n",
      "[[29.66193572]]\n",
      "[[82.41257727]]\n",
      "[[81.9165184]]\n",
      "[[81.52975133]]\n",
      "[[81.27852501]]\n",
      "[[80.86547207]]\n",
      "[[80.37996808]]\n",
      "[[78.55374779]]\n",
      "[[79.01579566]]\n",
      "[[82.37503835]]\n",
      "[[76.25059378]]\n",
      "[[75.25743764]]\n",
      "[[62.14532425]]\n",
      "[[60.899837]]\n",
      "[[61.36122294]]\n",
      "[[66.35071792]]\n",
      "[[60.61397035]]\n",
      "[[60.22719654]]\n",
      "[[58.96601296]]\n",
      "[[57.99911134]]\n",
      "[[57.88033961]]\n",
      "[[61.32256052]]\n",
      "[[55.0059067]]\n",
      "[[63.32203428]]\n",
      "[[62.91946094]]\n",
      "[[61.6740296]]\n",
      "[[62.52217207]]\n",
      "[[67.12476127]]\n",
      "[[61.49740244]]\n",
      "[[61.00138438]]\n",
      "[[60.01871636]]\n",
      "[[59.05181543]]\n",
      "[[58.07441232]]\n",
      "[[61.51662992]]\n",
      "[[64.05678409]]\n",
      "[[58.31955898]]\n",
      "[[57.91695449]]\n",
      "[[57.05827136]]\n",
      "[[57.51965423]]\n",
      "[[62.12228701]]\n",
      "[[56.49488891]]\n",
      "[[55.4187115]]\n",
      "[[54.43604447]]\n",
      "[[52.61051379]]\n",
      "[[52.49174137]]\n",
      "[[55.64698912]]\n",
      "[[55.51116386]]\n",
      "[[54.76392216]]\n",
      "[[54.76392216]]\n",
      "[[53.50267252]]\n",
      "[[53.96405113]]\n",
      "[[53.32605338]]\n",
      "[[57.59967586]]\n",
      "[[51.28297554]]\n",
      "[[49.44168916]]\n",
      "[[48.47479249]]\n",
      "[[48.36652398]]\n",
      "[[235.91746829]]\n",
      "[[230.28938076]]\n",
      "[[230.18004475]]\n",
      "[[229.79324484]]\n",
      "[[228.53201709]]\n",
      "[[228.74216631]]\n",
      "[[227.7584925]]\n",
      "[[226.80832529]]\n",
      "[[225.84141946]]\n",
      "[[224.749527]]\n",
      "[[223.7721269]]\n",
      "[[48.76217213]]\n",
      "[[48.37539916]]\n",
      "[[48.12418632]]\n",
      "[[46.75372913]]\n",
      "[[47.22560869]]\n",
      "[[46.83884498]]\n",
      "[[45.59344741]]\n",
      "[[49.883795]]\n",
      "[[43.67637309]]\n",
      "[[42.60024089]]\n",
      "[[41.61758936]]\n",
      "[[55.52981009]]\n",
      "[[60.12205825]]\n",
      "[[60.12205825]]\n",
      "[[54.4945159]]\n",
      "[[53.99846813]]\n",
      "[[53.59593212]]\n",
      "[[57.6078009]]\n",
      "[[52.23175743]]\n",
      "[[55.67397508]]\n",
      "[[49.3573219]]\n",
      "[[48.39042562]]\n",
      "[[118.03484511]]\n",
      "[[117.51236702]]\n",
      "[[122.50192336]]\n",
      "[[122.11505437]]\n",
      "[[116.48760294]]\n",
      "[[115.99156685]]\n",
      "[[115.58903318]]\n",
      "[[114.61162011]]\n",
      "[[113.64471983]]\n",
      "[[117.08693554]]\n",
      "[[110.77028751]]\n",
      "[[83.63294659]]\n",
      "[[83.23037141]]\n",
      "[[82.84360097]]\n",
      "[[82.83309017]]\n",
      "[[87.43566064]]\n",
      "[[81.80831365]]\n",
      "[[81.31230674]]\n",
      "[[80.32964585]]\n",
      "[[79.37850365]]\n",
      "[[77.55394108]]\n",
      "[[76.58704476]]\n",
      "[[68.30863775]]\n",
      "[[62.57148682]]\n",
      "[[61.31026873]]\n",
      "[[61.31026873]]\n",
      "[[61.77164298]]\n",
      "[[61.13363725]]\n",
      "[[60.74686849]]\n",
      "[[59.64445122]]\n",
      "[[58.68805664]]\n",
      "[[56.86252968]]\n",
      "[[55.9123538]]\n",
      "[[-1.24943925]]\n",
      "[[-1.73500345]]\n",
      "[[-1.74551841]]\n",
      "[[-3.56123973]]\n",
      "[[-2.5190489]]\n",
      "[[2.00050992]]\n",
      "[[-3.54382878]]\n",
      "[[-4.53699646]]\n",
      "[[-5.503896]]\n",
      "[[-8.0089715]]\n",
      "[[-8.37832901]]\n",
      "[[12.87090636]]\n",
      "[[12.48405351]]\n",
      "[[12.07094466]]\n",
      "[[11.97219859]]\n",
      "[[10.14596983]]\n",
      "[[11.18816012]]\n",
      "[[15.70770814]]\n",
      "[[9.58323683]]\n",
      "[[8.590074]]\n",
      "[[7.6231757]]\n",
      "[[5.11810173]]\n",
      "[[4.15120566]]\n",
      "[[15.66876418]]\n",
      "[[15.25569449]]\n",
      "[[15.25569444]]\n",
      "[[13.3307263]]\n",
      "[[12.94396611]]\n",
      "[[13.98615412]]\n",
      "[[12.76799221]]\n",
      "[[11.80109084]]\n",
      "[[15.99173197]]\n",
      "[[9.86729418]]\n",
      "[[7.33596357]]\n",
      "[[39.57051106]]\n",
      "[[44.09020745]]\n",
      "[[38.93253525]]\n",
      "[[38.51945932]]\n",
      "[[38.03394657]]\n",
      "[[36.20772218]]\n",
      "[[37.24991137]]\n",
      "[[36.39224771]]\n",
      "[[39.64225055]]\n",
      "[[33.49154894]]\n",
      "[[30.98647611]]\n",
      "[[68.03352848]]\n",
      "[[62.35304174]]\n",
      "[[67.25963277]]\n",
      "[[61.59000953]]\n",
      "[[61.30198674]]\n",
      "[[60.81646805]]\n",
      "[[58.99023696]]\n",
      "[[59.45228498]]\n",
      "[[62.81153366]]\n",
      "[[56.68708151]]\n",
      "[[55.69392428]]\n",
      "[[76.95095375]]\n",
      "[[76.4653843]]\n",
      "[[76.06808785]]\n",
      "[[75.81688135]]\n",
      "[[75.43009595]]\n",
      "[[75.01703721]]\n",
      "[[74.53153114]]\n",
      "[[73.6896267]]\n",
      "[[71.15827449]]\n",
      "[[70.78891753]]\n",
      "[[69.82202106]]\n",
      "[[36.40294605]]\n",
      "[[35.9174226]]\n",
      "[[35.65567345]]\n",
      "[[40.42648755]]\n",
      "[[34.88213217]]\n",
      "[[34.49536801]]\n",
      "[[34.10860559]]\n",
      "[[33.01671239]]\n",
      "[[30.61036741]]\n",
      "[[35.39854636]]\n",
      "[[30.10551807]]\n",
      "[[70.46375261]]\n",
      "[[70.07697518]]\n",
      "[[69.58095593]]\n",
      "[[68.15200506]]\n",
      "[[67.76524491]]\n",
      "[[68.80743248]]\n",
      "[[68.16941235]]\n",
      "[[67.17624848]]\n",
      "[[66.1106201]]\n",
      "[[65.13321956]]\n",
      "[[64.16632299]]\n",
      "[[43.57132973]]\n",
      "[[43.18440091]]\n",
      "[[37.61351254]]\n",
      "[[37.51475924]]\n",
      "[[37.11747616]]\n",
      "[[36.47947292]]\n",
      "[[34.9149951]]\n",
      "[[35.12578803]]\n",
      "[[34.13261906]]\n",
      "[[33.06698885]]\n",
      "[[30.66064387]]\n",
      "[[31.12269149]]\n",
      "[[17.71720038]]\n",
      "[[12.1724565]]\n",
      "[[12.14613497]]\n",
      "[[11.66060117]]\n",
      "[[9.83436365]]\n",
      "[[15.78290841]]\n",
      "[[14.81596148]]\n",
      "[[8.66522579]]\n",
      "[[7.6983262]]\n",
      "[[6.63269797]]\n",
      "[[5.6658017]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(na_row_without_na_vmaf), 7):\n",
    "    print(LR.predict([na_row_without_na_vmaf.to_numpy()[i]])[0:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.38927837233621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.6669855554281"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.mean())\n",
    "y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "\n",
    "# max_features is optimum at 41, but could be less to improve time and lose some score.\n",
    "rf_model_pre = RandomForestRegressor(\n",
    "            n_estimators = 25,\n",
    "            criterion    = 'mse',\n",
    "            max_depth    = None,\n",
    "            max_features = 41,\n",
    "            oob_score    = True,\n",
    "            n_jobs    shape= -1,\n",
    "            random_state = 123\n",
    "         )\n",
    "# max_features is optimum at 300+ but 75 seems reasonable, yet it could be less to improve time and lose some score.\n",
    "rf_model_aug = RandomForestRegressor(\n",
    "            n_estimators = 25,\n",
    "            criterion    = 'mse',\n",
    "            max_depth    = None,\n",
    "            max_features = 75,\n",
    "            oob_score    = True,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 123\n",
    "         )\n",
    "# max_features is optimum at 5 and it cannot be higher than that. Increasing n_estimators doesn't help much.\n",
    "rf_model_pca = RandomForestRegressor(\n",
    "            n_estimators = 25,\n",
    "            criterion    = 'mse',\n",
    "            max_depth    = None,\n",
    "            max_features = 5,\n",
    "            oob_score    = True,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 123\n",
    "         )\n",
    "print(run_model(rf_model_pre, x_train_pre, y_train_pre, x_test_pre, y_test_pre))\n",
    "print(run_model(rf_model_aug, x_train_aug, y_train_aug, x_test_aug, y_test_aug))\n",
    "print(run_model(rf_model_pca, x_train_pca, y_train_pca, x_test_pca, y_test_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# single layer perceptron model in keras\n",
    "# input is the number of columns\n",
    "# there is one fully connected layer with 128 nodes, 128 is arbitrary \n",
    "# output is one float\n",
    "def make_nn1_closure(firstLayerNodes, dropOutPercentage, optimiser, activation, input_dim):\n",
    "    def make_nn1():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(firstLayerNodes, activation=activation, input_dim=input_dim))\n",
    "        model.add(Dropout(dropOutPercentage))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        # plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "        return model\n",
    "    return make_nn1\n",
    "\n",
    "def make_nn2_closure(firstLayerNodes, secondLayerNodes, dropOutPercentage, optimiser, activation, input_dim):\n",
    "    def make_nn2():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(firstLayerNodes, activation=activation, input_dim=input_dim))\n",
    "        model.add(Dropout(dropOutPercentage))\n",
    "        model.add(Dense(secondLayerNodes, activation=activation))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        # plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "        return model\n",
    "    return make_nn2\n",
    "\n",
    "class Params:\n",
    "        \n",
    "        def __init__(self, first_nodes, second_nodes, epochs, kfold, opti, acti, drop, batch_size):\n",
    "            self.first_nodes = first_nodes\n",
    "            self.second_nodes = second_nodes\n",
    "            self.epochs = epochs\n",
    "            self.kfold = kfold\n",
    "            self.optimizer = opti\n",
    "            self.activation = acti\n",
    "            self.drop = drop\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        def __str__(self):\n",
    "            attrs = vars(self)\n",
    "            return ', '.join(\"%s: %s\" % item for item in attrs.items())\n",
    "\n",
    "class Grid_parameters:\n",
    "    nodeValues = list(range(5, 50))\n",
    "    batchValues = [2, 4, 8, 16, 32]\n",
    "    epochesValues = [2]#list(range(35, 75, 10))\n",
    "    kFoldSplitValues = [10, 5, 12]\n",
    "    optimizers = [\"RMSprop\", \"adam\", \"Adadelta\", \"Nadam\"]\n",
    "    activations=[\"softmax\", \"relu\", \"tanh\"]\n",
    "    dropOuts = list(np.arange(0.0, 0.8, 0.05))\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_random_params(self):\n",
    "        return Params(random.choice(self.nodeValues), \n",
    "                      random.choice(self.nodeValues),\n",
    "                      random.choice(self.epochesValues),\n",
    "                      random.choice(self.kFoldSplitValues),\n",
    "                      random.choice(self.optimizers),\n",
    "                      random.choice(self.activations),\n",
    "                      random.choice(self.dropOuts),\n",
    "                      random.choice(self.batchValues)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Grid_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-512-9f32aedd2bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#infile.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrid_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mresults_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Grid_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "# cross validating the model doesnt fit it, i.e. cannot be used to predict\n",
    "# need to regualrly fit it using all the training data\n",
    "# cv usefull for selecting the model topology\n",
    "\n",
    "# PICKLE READ FILE\n",
    "\n",
    "#infile = open('results_pre','rb')\n",
    "#new_dict = pickle.load(infile)\n",
    "#print(new_dict[0][0])\n",
    "#infile.close()\n",
    "\n",
    "grid = Grid_parameters()\n",
    "\n",
    "results_pre = []\n",
    "results_aug = []\n",
    "results_pca = []\n",
    "\n",
    "for i in range(0, 20):\n",
    "    params = grid.get_random_params()\n",
    "    print(params)\n",
    "    \n",
    "    estimators_1 = []\n",
    "    estimators_2 = []\n",
    "    estimators_1.append(('standardize', StandardScaler()))\n",
    "    estimators_2.append(('standardize', StandardScaler()))\n",
    "    \n",
    "    model_1 = make_nn1_closure(params.first_nodes, params.drop, params.optimizer, params.activation, x_train_pre.shape[1])\n",
    "    model_2 = make_nn2_closure(params.first_nodes, params.second_nodes, params.drop, params.optimizer, params.activation, x_train_pre.shape[1])\n",
    "    \n",
    "    estimators_1.append(('mlp', KerasRegressor(build_fn=model_1, \n",
    "                                             epochs=params.epochs, \n",
    "                                             batch_size=params.batch_size, \n",
    "                                             verbose=1)))\n",
    "    estimators_2.append(('mlp', KerasRegressor(build_fn=model_2, \n",
    "                                             epochs=params.epochs, \n",
    "                                             batch_size=params.batch_size, \n",
    "                                             verbose=1)))\n",
    "    \n",
    "    pipeline_1 = Pipeline(estimators_1)\n",
    "    pipeline_1 = Pipeline(estimators_2)\n",
    "    \n",
    "    kfold = KFold(n_splits=params.kfold)\n",
    "    \n",
    "    result_pre_1 = cross_val_score(pipeline_1, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    result_pre_2 = cross_val_score(pipeline_2, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    #result_aug = cross_val_score(pipeline, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    #result_pca = cross_val_score(pipeline, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    \n",
    "    print(\"Standardized: mean is %.2f, std is %.2f\" % (result_pre.mean(), result_pre.std()))\n",
    "    #print(\"Standardized: mean is %.2f, std is %.2f\" % (result_aug.mean(), result_aug.std()))\n",
    "    #print(\"Standardized: mean is %.2f, std is %.2f\" % (result_pca.mean(), result_pca.std()))\n",
    "    \n",
    "    results_pre.append((params, result_pre))\n",
    "    #results_aug.append((params, result_aug))\n",
    "    #results_pca.append((params, result_pca))\n",
    "    \n",
    "    with open('results_pre', 'wb') as save:\n",
    "        pickle.dump(results_pre, save)\n",
    "    \n",
    "    \n",
    "    \n",
    "# below I copied pased the best results \n",
    "\n",
    "# No Dropout\n",
    "# Standardized: mean is -46.98, std is 18.11\n",
    "            # running model with 29 firstLayerNodes, 8 secondLayerNodes,           \n",
    "            # 40 epochs, 10 batchSize, 10 k_fold_split\n",
    "\n",
    "# Standardized: mean is -48.66, std is 18.17\n",
    "            # running model with 29 firstLayerNodes, 44 secondLayerNodes,           \n",
    "            # 40 epochs, 5 batchSize, 5 k_fold_split\n",
    "\n",
    "# Standardized: mean is -51.15, std is 14.19, \n",
    "            # running model with 6 firstLayerNodes, 50 secondLayerNodes, \n",
    "            # 30 epochs, 20 batchSize, 10 k_fold_split\n",
    "        \n",
    "# Dropout after first dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that content_category features have high scores on average.\n",
    "# Unfortunately there are many content_categories and often \n",
    "# there is only one representative movie for each one.\n",
    "# This means that is a powerful feature but it doesnt generalize.\n",
    "# (e.g. security example: if I see a man with a gun is 100% dangerous but it's rare that such obvious feature\n",
    "# will be observed; many men are dangerous without showing a gun)\n",
    "\n",
    "categoriesWithHighWeight = [\"c_content_category_croquet ball\", \n",
    "                            \"c_content_category_bathing cap\", \n",
    "                            \"c_content_category_dumbbell\",\n",
    "                           \"c_content_category_envelope\",\n",
    "                           \"c_content_category_palace\"]\n",
    "for cat in categoriesWithHighWeight:\n",
    "    howManyRowsHaveTheCategory = sum(data[cat].tolist())\n",
    "    print(\"There are {0} raws with category {1}\".format(howManyRowsHaveTheCategory, cat))\n",
    "    categoryIndexer = (data[cat] == 1).tolist()\n",
    "    videoIdsCorrespondingToCategory = data[\"s_video_id\"][categoryIndexer]\n",
    "    boolean_array_are_some_ids_different_from_first = videoIdsCorrespondingToCategory != videoIdsCorrespondingToCategory.tolist()[0]\n",
    "    at_least_two_ids_are_different = any(boolean_array_are_some_ids_different_from_first.tolist())\n",
    "    if at_least_two_ids_are_different:\n",
    "        print(\"{0} is found on more than one movie\\n\".format(cat))\n",
    "    else :\n",
    "        print(\"{0} is found on EXACTLY one movie\\n\".format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now the parameters closest to 0\n",
    "epsilon = 0.1\n",
    "dfParam[((regular_regression.coef_ < epsilon) & (regular_regression.coef_ > -epsilon)).tolist()[0]].sort_values(by='Weight', ascending=False)\n",
    "\n",
    "# TODO remove e_scan_type_not supported yet? what does this column actually mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between most relevant parameters and t_average_vmaf using scatterplots\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "sns.regplot(x='c_content_category_croquet ball', y='t_average_vmaf', data=data, ax=axs[0])\n",
    "sns.regplot(x='c_colorhistogram_std_dev_medium_bright', y='t_average_vmaf', data=data, ax=axs[1])\n",
    "sns.regplot(x='c_content_category_bathing cap',y='t_average_vmaf', data=data, ax=axs[2])\n",
    "sns.regplot(x='c_colorhistogram_mean_bright',y='t_average_vmaf', data=data, ax=axs[3])\n",
    "plt.show()\n",
    "\n",
    "# Nothing useful.... I don't delete it in case we can use this tool in the future for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the correlation of the 2 most relevant (positive and negative) non-categorical variables\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "sns.regplot(x='c_colorhistogram_std_dev_medium_bright', y='c_colorhistogram_mean_bright', data=data, ax=axs[0]) #Positive\n",
    "sns.regplot(x='c_colorhistogram_std_dev_medium_dark', y='c_colorhistogram_mean_medium_bright', data=data, ax=axs[1]) #Negative\n",
    "plt.show()\n",
    "\n",
    "# Nothing useful... I don't delete it in case we can use this tool in the future for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We repeat the regular regression but we remove the content_category from the taining data\n",
    "# Purpose\n",
    "# 1: experiment and see how model performs without such scattered information\n",
    "# 2: force the model to give importance to features that generalise\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "columns_except_content_category = [col for col in x_train.columns if not 'content_category' in col]\n",
    "\n",
    "regular_regression2 = LinearRegression().fit(x_train[columns_except_content_category].to_numpy(), y_train.to_numpy())\n",
    "coefficient_of_determination2 = regular_regression2.score(x_test[columns_except_content_category].to_numpy(), y_test.to_numpy())\n",
    "print(\"Coefficient of determination on unseen test data {0}\".format(coefficient_of_determination2))\n",
    "\n",
    "predictions2 = regular_regression2.predict(x_test[columns_except_content_category].to_numpy())\n",
    "mse2 = mean_squared_error(y_test, predictions2)\n",
    "print(\"MSE on test {0}\".format(mse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importance of predictors by purity of nodes\n",
    "# ==============================================================================\n",
    "predictors_importance = pd.DataFrame(\n",
    "                            {'predictor': data.drop(columns = \"t_average_vmaf\").columns,\n",
    "                             'importance': RFmodel.feature_importances_}\n",
    "                            )\n",
    "print(\"importance of predictors\")\n",
    "print(\"-------------------------------------------\")\n",
    "predictors_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# TODO: why is height & width so important for this model but it wasnt at all in linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance of predictors by permutation\n",
    "# ==============================================================================\n",
    "from sklearn.inspection import permutation_importance\n",
    "import multiprocessing\n",
    "\n",
    "importance = permutation_importance(\n",
    "                estimator    = RFmodel,\n",
    "                X            = x_train,\n",
    "                y            = y_train,\n",
    "                n_repeats    = 5,\n",
    "                scoring      = 'neg_root_mean_squared_error',\n",
    "                n_jobs       = multiprocessing.cpu_count() - 1,\n",
    "                random_state = 123\n",
    "             )\n",
    "\n",
    "# Store results (mean and stddev) in a dataframe\n",
    "df_importance = pd.DataFrame(\n",
    "                    {k: importance[k] for k in ['importances_mean', 'importances_std']}\n",
    "                 )\n",
    "df_importance['feature'] = x_train.columns\n",
    "df_importance.sort_values('importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "\n",
    "# we can see how the content_categories feature have very little importance.\n",
    "# This is expected since creating a 'split' on a feture that is valorised only for\n",
    "# a small number of datapoints is against the criteria of how random forest works\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 18))\n",
    "df_importance = df_importance.sort_values('importances_mean', ascending=True)\n",
    "ax.barh(\n",
    "    df_importance['feature'],\n",
    "    df_importance['importances_mean'],\n",
    "    xerr=df_importance['importances_std'],\n",
    "    align='center',\n",
    "    alpha=0\n",
    ")\n",
    "ax.plot(\n",
    "    df_importance['importances_mean'],\n",
    "    df_importance['feature'],\n",
    "    marker=\"D\",\n",
    "    linestyle=\"\",\n",
    "    alpha=0.8,\n",
    "    color=\"r\"\n",
    ")\n",
    "ax.set_title('Importance of predictors (train)')\n",
    "ax.set_xlabel('Error increase after permutation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:  # Skip it, this cell is used only to obtain the optimum value of n_estimators through out-of-bag error\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Validation using Out-of-Bag error\n",
    "    # ==============================================================================\n",
    "    train_scores = []\n",
    "    oob_scores   = []\n",
    "    iter_x_train = x_train_pca\n",
    "    iter_y_train = y_train_pca\n",
    "\n",
    "    # Evaluated values\n",
    "    estimator_range = range(1, 50, 2)\n",
    "\n",
    "    # Loop to train model with each n_estimators value and obtain its training and OOB error\n",
    "    for n_estimators in estimator_range:\n",
    "        RFmodel = RandomForestRegressor(\n",
    "                    n_estimators = n_estimators,\n",
    "                    criterion    = 'mse',\n",
    "                    max_depth    = None,\n",
    "                    max_features = 'auto',\n",
    "                    oob_score    = True,\n",
    "                    n_jobs       = -1,\n",
    "                    random_state = 123\n",
    "                 )\n",
    "        RFmodel.fit(iter_x_train, iter_y_train)\n",
    "        train_scores.append(RFmodel.score(iter_x_train, iter_y_train))\n",
    "        oob_scores.append(RFmodel.oob_score_)\n",
    "\n",
    "    # Graph with the error trend\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "    ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "    ax.plot(estimator_range, oob_scores, label=\"out-of-bag scores\")\n",
    "    ax.plot(estimator_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "            marker='o', color = \"red\", label=\"max score\")\n",
    "    ax.set_ylabel(\"R^2\")\n",
    "    ax.set_xlabel(\"n_estimators\")\n",
    "    ax.set_title(\"out-of-bag-error evolution vs number of trees\")\n",
    "    plt.legend();\n",
    "    print(f\"Max score: {max(oob_scores)}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:  # Skip it, this cell is used only to obtain the optimum value of n_estimators through k-cross-validation and neg_root_mean_squared_error\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Validation using k-cross-validation and neg_root_mean_squared_error\n",
    "    # ==============================================================================\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    train_scores = []\n",
    "    cv_scores    = []\n",
    "    iter_x_train = x_train_pca\n",
    "    iter_y_train = y_train_pca\n",
    "\n",
    "    # Evaluated values\n",
    "    estimator_range = range(1, 30, 2)\n",
    "\n",
    "    # Loop to train a model with each n_estimators value and obtain its training and\n",
    "    # k-cross-validation error.\n",
    "    for n_estimators in estimator_range:\n",
    "\n",
    "        RFmodel = RandomForestRegressor(\n",
    "                    n_estimators = n_estimators,\n",
    "                    criterion    = 'mse',\n",
    "                    max_depth    = None,\n",
    "                    max_features = 'auto',\n",
    "                    oob_score    = False,\n",
    "                    n_jobs       = -1,\n",
    "                    random_state = 123\n",
    "                 )\n",
    "\n",
    "        # Train error\n",
    "        RFmodel.fit(iter_x_train, iter_y_train)\n",
    "        RFpredictions = RFmodel.predict(X = iter_x_train)\n",
    "        rmse = mean_squared_error(\n",
    "                y_true  = iter_y_train,\n",
    "                y_pred  = RFpredictions,\n",
    "                squared = False\n",
    "               )\n",
    "        train_scores.append(rmse)\n",
    "\n",
    "        # Cross validation error\n",
    "        RFscores = cross_val_score(\n",
    "                    estimator = RFmodel,\n",
    "                    X         = iter_x_train,\n",
    "                    y         = iter_y_train,\n",
    "                    scoring   = 'neg_root_mean_squared_error',\n",
    "                    cv        = 5\n",
    "                 )\n",
    "        # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
    "        cv_scores.append(-1*RFscores.mean())\n",
    "\n",
    "    # Graph with the error trend\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "    ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "    ax.plot(estimator_range, cv_scores, label=\"cv scores\")\n",
    "    ax.plot(estimator_range[np.argmin(cv_scores)], min(cv_scores),\n",
    "            marker='o', color = \"red\", label=\"min score\")\n",
    "    ax.set_ylabel(\"root_mean_squared_error\")\n",
    "    ax.set_xlabel(\"n_estimators\")\n",
    "    ax.set_title(\"cv-error evolution vs tree number\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:  # Skip it, this cell is used only to obtain the optimum value of max_features through Out-of-Bag error\n",
    "    \n",
    "    # Validation using Out-of-Bag error\n",
    "    # ==============================================================================\n",
    "    import math\n",
    "\n",
    "    train_scores = []\n",
    "    oob_scores   = []\n",
    "    iter_x_train = x_train_pca\n",
    "    iter_y_train = y_train_pca\n",
    "\n",
    "    # Evaluated values\n",
    "    max_features_range = range(1, 2*math.ceil(iter_x_train.shape[1]/2), 1)\n",
    "\n",
    "    # Loop to train a model with each max_features value and obtain its training and\n",
    "    # Out-of-Bag error.\n",
    "    for max_features in max_features_range:\n",
    "        RFmodel = RandomForestRegressor(\n",
    "                    n_estimators = 25,\n",
    "                    criterion    = 'mse',\n",
    "                    max_depth    = None,\n",
    "                    max_features = max_features,\n",
    "                    oob_score    = True,\n",
    "                    n_jobs       = -1,\n",
    "                    random_state = 123\n",
    "                 )\n",
    "        RFmodel.fit(iter_x_train, iter_y_train)\n",
    "        train_scores.append(RFmodel.score(iter_x_train, iter_y_train))\n",
    "        oob_scores.append(RFmodel.oob_score_)\n",
    "\n",
    "    # Graph with the error trend\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "    ax.plot(max_features_range, train_scores, label=\"train scores\")\n",
    "    ax.plot(max_features_range, oob_scores, label=\"out-of-bag scores\")\n",
    "    ax.plot(max_features_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "            marker='o', color = \"red\")\n",
    "    ax.set_ylabel(\"R^2\")\n",
    "    ax.set_xlabel(\"max_features\")\n",
    "    ax.set_title(\"out-of-bag-error evolution vs number of predictors\")\n",
    "    plt.legend();\n",
    "    print(f\"optimum value of max_features: {max_features_range[np.argmax(oob_scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
