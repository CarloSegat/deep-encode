{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random \n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def print_data_summary(data):\n",
    "    print(\"There are {0} datapoints\".format(len(data)))\n",
    "    print(\"There are {0} columns\".format(len(data.columns)))\n",
    "    try:\n",
    "        print(\"There are {0} diffenret movies\".format(len(set(data['s_video_id']))))\n",
    "    except:\n",
    "        pass\n",
    "    # print(data.describe())\n",
    "    \n",
    "def encode_one_hot(data):\n",
    "    categorical_columns = get_categorical_columns(data)\n",
    "    oneHotEncodedData = pd.get_dummies(data, columns=categorical_columns)\n",
    "    \n",
    "    # Test\n",
    "    newNumberOfColumns = len(oneHotEncodedData.columns) \n",
    "\n",
    "    totalDifferentValues = 0\n",
    "    for c in categorical_columns:\n",
    "        totalDifferentValues += len(data[c].unique())\n",
    "\n",
    "    # One hot encoding removes each categorical columns (- len(nonNumericalList)) \n",
    "    # and adds n columns where n is number of different values taken by column (+ totalDifferentValues)\n",
    "    assert len(data.columns) + totalDifferentValues - len(categorical_columns) == newNumberOfColumns\n",
    "    \n",
    "    return oneHotEncodedData\n",
    "\n",
    "def split_train_test(data, test_mask):\n",
    "    # split between train and test\n",
    "    train_mask = list(map(lambda b: not b, test_mask))\n",
    "\n",
    "    train = data[train_mask]\n",
    "    test = data[test_mask]\n",
    "\n",
    "    assert len(train) + len(test) == len(data) \n",
    "\n",
    "    y_train = train.loc[:, ['t_average_vmaf']]\n",
    "    x_train = train.drop(['t_average_vmaf'], axis=1)\n",
    "    assert type(x_train) == type(y_train) \n",
    "\n",
    "    y_test = test.loc[:, ['t_average_vmaf']]\n",
    "    x_test = test.drop(['t_average_vmaf'], axis=1)\n",
    "    assert type(x_test) == type(y_test)\n",
    "\n",
    "    assert len(x_train) == len(y_train)\n",
    "    assert len(x_train.columns) + len(y_train.columns) == len(data.columns)\n",
    "    assert len(x_test) == len(y_test)\n",
    "    assert len(x_test.columns) + len(y_test.columns) == len(data.columns)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def calculate_rmse(y_true, y_predictions):\n",
    "    mse = mean_squared_error(y_true, y_predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def calculate_std(y_true, y_predictions):\n",
    "    '''\n",
    "    Standard deviation is calculated considering the absoulte values of the residuals as the\n",
    "    values and the RMSE as the mean.\n",
    "    '''\n",
    "    rmse = calculate_rmse(y_true, y_predictions)\n",
    "    \n",
    "    abs_residuals = np.absolute(y_true - y_predictions)\n",
    "    \n",
    "    \n",
    "    #print(abs_residuals)\n",
    "    #print(abs_residuals.tolist().sort(reverse=True))\n",
    "    tbp = list(np.ravel(abs_residuals))\n",
    "    tbp.sort(reverse=True)\n",
    "    print(\"Printing the 4th quartile to doublecheck std: \")\n",
    "    print(np.percentile(tbp, 75))  # Q3\n",
    "    #print(tbp[0:20])\n",
    "\n",
    "    \n",
    "    rmse_column = np.array([rmse]*len(abs_residuals)).reshape(len(abs_residuals), 1)\n",
    "\n",
    "    #print(\"len(abs_residuals)\")\n",
    "    #print(len(abs_residuals))\n",
    "    #print(\"rmse_column\")\n",
    "    #print(rmse_column)\n",
    "    variance = np.sum(np.square(abs_residuals - rmse_column)) / len(abs_residuals)\n",
    "    std = math.sqrt(variance)\n",
    "    return std\n",
    "\n",
    "def run_model(regression_model, x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Given a sklearn model and the train / test data,\n",
    "    returns the rmse, std and r squared scores of the model\n",
    "    also returns training time in seconds\n",
    "    '''\n",
    "    start_time = datetime.now().timestamp()\n",
    "   \n",
    "    regular_regression = regression_model.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "    end_time = datetime.now().timestamp()\n",
    "   \n",
    "    predictions = regression_model.predict(x_test.to_numpy())\n",
    "\n",
    "    rmse = calculate_rmse(y_test.to_numpy(), predictions)\n",
    "\n",
    "    std = calculate_std(y_test.to_numpy(), predictions)\n",
    "\n",
    "    coefficient_of_determination = regression_model.score(x_test.to_numpy(), y_test.to_numpy())\n",
    "    \n",
    "    return Model_results(rmse, std, coefficient_of_determination, end_time - start_time, regression_model)\n",
    "\n",
    "def analyse_model(regression_model, x_train, y_train, x_test, y_test):\n",
    "    regular_regression = regression_model.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "    \n",
    "    paramRelevance = {'Parameter':x_train.columns.to_list()[:],\n",
    "            'Weight':regular_regression.coef_.tolist()[0]}\n",
    " \n",
    "    dfParam = pd.DataFrame(paramRelevance)\n",
    "\n",
    "    print(dfParam.sort_values(by='Weight', ascending=False)[:])\n",
    "\n",
    "class Model_results:\n",
    "    def __init__(self, rmse, std, cod, time, model):\n",
    "        self.rmse = rmse\n",
    "        self.std = std\n",
    "        self.cod = cod\n",
    "        self.time = time\n",
    "        self.model = model\n",
    "        \n",
    "    def __str__(self):\n",
    "        attrs = vars(self)\n",
    "        return ', '.join(\"%s: %s\" % item for item in attrs.items())\n",
    "    \n",
    "def get_numerical_columns(data):\n",
    "    numerical = data.select_dtypes(exclude=['object'])\n",
    "    exclude_categorical = [col for col in numerical.columns \\\n",
    "                                   if not 'content_category' in col \\\n",
    "                                   and not 'scan_type' in col \\\n",
    "                                   and not 'codec_profile' in col \\\n",
    "                                   and not 's_video_id' in col]\n",
    "    return exclude_categorical\n",
    "\n",
    "def get_categorical_columns(data):\n",
    "    numerical_columns = set(get_numerical_columns(data))\n",
    "    return set(data.columns) - numerical_columns\n",
    "\n",
    "def explore_categorical_features(data):\n",
    "    # explorig categorical columns\n",
    "    nonNumericalFrame = data.select_dtypes(include=['object']).copy()\n",
    "    nonNumericalList = nonNumericalFrame.columns.tolist()\n",
    "    totalDifferentValues = 0\n",
    "    for c in nonNumericalList:\n",
    "        print(\"{0} is non-numerical and values are: {1}\".format(c, nonNumericalFrame[c].unique()))\n",
    "        totalDifferentValues += len(nonNumericalFrame[c].unique())\n",
    "\n",
    "    print(\"There are {0} total possible values among the categorical columns\".format(totalDifferentValues))\n",
    "\n",
    "    # content_category is the most problematic column because it has many possible values\n",
    "    # nonNumericalFrame[\"c_content_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "rawColumns = data.columns\n",
    "rawData = deepcopy(data)\n",
    "\n",
    "# test \n",
    "assert len(get_numerical_columns(data)) + len(get_categorical_columns(data)) == len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns have been removed because empty: {'t_average_vmaf_mobile', 't_average_vmaf_4k', 't_average_psnr'}\n",
      "There were 2883 rows deleted because they had less than 43 valorised features\n",
      "Column e_aspect_ratio always has same value: 16:09, removed\n",
      "Column e_pixel_aspect_ratio always has same value: 1:01, removed\n",
      "Column e_codec always has same value: h264, removed\n",
      "Column e_b_frame_int always has same value: 3, removed\n",
      "Column e_ref_frame_count always has same value: 1.0, removed\n",
      "Column e_bit_depth always has same value: 8.0, removed\n",
      "Column e_pixel_fmt always has same value: yuv420p, removed\n",
      "\n",
      "After inital processing\n",
      "There are 12133 datapoints\n",
      "There are 35 columns\n",
      "\n",
      "After scaling\n",
      "There are 12133 datapoints\n",
      "There are 34 columns\n",
      "\n",
      "After 1 hot encoding categorical features\n",
      "There are 12133 datapoints\n",
      "There are 110 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_storage_size</th>\n",
       "      <th>s_duration</th>\n",
       "      <th>c_si</th>\n",
       "      <th>c_ti</th>\n",
       "      <th>c_scene_change_ffmpeg_ratio30</th>\n",
       "      <th>c_scene_change_ffmpeg_ratio60</th>\n",
       "      <th>c_scene_change_ffmpeg_ratio90</th>\n",
       "      <th>c_scene_change_py_thresh30</th>\n",
       "      <th>c_scene_change_py_thresh50</th>\n",
       "      <th>c_colorhistogram_mean_dark</th>\n",
       "      <th>...</th>\n",
       "      <th>e_codec_profile_high</th>\n",
       "      <th>e_codec_profile_main</th>\n",
       "      <th>s_size_1080_1080</th>\n",
       "      <th>s_size_1280_720</th>\n",
       "      <th>s_size_1920_1080</th>\n",
       "      <th>s_size_1920_800</th>\n",
       "      <th>s_size_1920_818</th>\n",
       "      <th>s_size_352_288</th>\n",
       "      <th>s_size_720_486</th>\n",
       "      <th>s_size_720_608</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   s_storage_size  s_duration      c_si     c_ti  \\\n",
       "0        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "1        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "2        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "3        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "4        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "\n",
       "   c_scene_change_ffmpeg_ratio30  c_scene_change_ffmpeg_ratio60  \\\n",
       "0                      -0.239026                       -0.74125   \n",
       "1                      -0.239026                       -0.74125   \n",
       "2                      -0.239026                       -0.74125   \n",
       "3                      -0.239026                       -0.74125   \n",
       "4                      -0.239026                       -0.74125   \n",
       "\n",
       "   c_scene_change_ffmpeg_ratio90  c_scene_change_py_thresh30  \\\n",
       "0                      -0.512497                   -1.518056   \n",
       "1                      -0.512497                   -1.518056   \n",
       "2                      -0.512497                   -1.518056   \n",
       "3                      -0.512497                   -1.518056   \n",
       "4                      -0.512497                   -1.518056   \n",
       "\n",
       "   c_scene_change_py_thresh50  c_colorhistogram_mean_dark  ...  \\\n",
       "0                   -1.161942                   -0.214322  ...   \n",
       "1                   -1.161942                   -0.214322  ...   \n",
       "2                   -1.161942                   -0.214322  ...   \n",
       "3                   -1.161942                   -0.214322  ...   \n",
       "4                   -1.161942                   -0.214322  ...   \n",
       "\n",
       "   e_codec_profile_high  e_codec_profile_main  s_size_1080_1080  \\\n",
       "0                     1                     0                 0   \n",
       "1                     0                     1                 0   \n",
       "2                     0                     1                 0   \n",
       "3                     0                     1                 0   \n",
       "4                     0                     1                 0   \n",
       "\n",
       "   s_size_1280_720  s_size_1920_1080  s_size_1920_800  s_size_1920_818  \\\n",
       "0                0                 1                0                0   \n",
       "1                0                 1                0                0   \n",
       "2                0                 1                0                0   \n",
       "3                0                 1                0                0   \n",
       "4                0                 1                0                0   \n",
       "\n",
       "   s_size_352_288  s_size_720_486  s_size_720_608  \n",
       "0               0               0               0  \n",
       "1               0               0               0  \n",
       "2               0               0               0  \n",
       "3               0               0               0  \n",
       "4               0               0               0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# 1 Remove empty columns\n",
    "data = data.dropna(axis=1, thresh=len(data.columns))\n",
    "print(\"The following columns have been removed because empty: {0}\".format(set(rawColumns) - set(data.columns)))\n",
    "\n",
    "# 2 Remove rows with missing values\n",
    "threshold = len(data.columns)\n",
    "data = data.dropna(axis=0, thresh=threshold)\n",
    "print(\"There were {0} rows deleted because they had less than {1} valorised features\".format(len(rawData) - len(data), threshold))\n",
    "\n",
    "# 3 Remove columns that always have the same value\n",
    "nunique = data.apply(pd.Series.nunique)\n",
    "cols_to_drop = nunique[nunique == 1].index\n",
    "data = data.drop(cols_to_drop, axis=1)\n",
    "for c in cols_to_drop.tolist():\n",
    "    print(\"Column {0} always has same value: {1}, removed\".format(c, rawData[c].tolist()[1]))\n",
    "assert len(nunique) == len(data.columns) + len(cols_to_drop)\n",
    "\n",
    "# 4 Remove video id\n",
    "data = data.drop(['s_video_id'], axis = 1)\n",
    "\n",
    "print(\"\\nAfter inital processing\")\n",
    "print_data_summary(data)\n",
    "\n",
    "# 5 Coalesce width and height to a categorical feature\n",
    "# print(data.groupby(['s_width','s_height']).size().reset_index().rename(columns={0:'count'}))\n",
    "\n",
    "data['s_size'] = data.apply(lambda row: str(row['s_width']) + \"_\" + str(row['s_height']), axis=1)\n",
    "data = data.drop(['s_width', 's_height'], axis = 1)\n",
    "\n",
    "# 6 convert size to \n",
    "\n",
    "# explore_categorical_features(data)\n",
    "\n",
    "# 6 Scale data (mean is zero)\n",
    "\n",
    "# split data in numerical, categorical, y columns\n",
    "y = data['t_average_vmaf']\n",
    "categorical = data[get_categorical_columns(data.drop(['t_average_vmaf'], axis = 1))]\n",
    "numerical = data[get_numerical_columns(data.drop(['t_average_vmaf'], axis = 1))]\n",
    "\n",
    "# scale only numerical\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numerical)\n",
    "scaled_numerical = scaler.transform(numerical)\n",
    "\n",
    "# recombine everything\n",
    "numerical_data = pd.DataFrame(scaled_numerical, numerical.index, columns=numerical.columns)\n",
    "# test\n",
    "assert len(numerical_data.columns) + len(categorical.columns) + 1 == len(data.columns)\n",
    "data = pd.concat([numerical_data, categorical, y], axis=1)\n",
    "\n",
    "print(\"\\nAfter scaling\")\n",
    "print_data_summary(data)\n",
    "\n",
    "# 5 One hot encoding categorical columns\n",
    "\n",
    "# pro wrt label encoding: the model wont derive false relations between columns based on numerical values (2 > 1)\n",
    "# con wrt label encoding: more columns are introduced\n",
    "\n",
    "data = encode_one_hot(data)\n",
    "print(\"\\nAfter 1 hot encoding categorical features\")\n",
    "print_data_summary(data)\n",
    "\n",
    "\n",
    "preprocessed_data = deepcopy(data)\n",
    "\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original oclumns 37\n",
      "After augmenting the data\n",
      "There are 12133 datapoints\n",
      "There are 850 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_storage_size</th>\n",
       "      <th>s_duration</th>\n",
       "      <th>c_si</th>\n",
       "      <th>c_ti</th>\n",
       "      <th>c_scene_change_ffmpeg_ratio30</th>\n",
       "      <th>c_scene_change_ffmpeg_ratio60</th>\n",
       "      <th>c_scene_change_ffmpeg_ratio90</th>\n",
       "      <th>c_scene_change_py_thresh30</th>\n",
       "      <th>c_scene_change_py_thresh50</th>\n",
       "      <th>c_colorhistogram_mean_dark</th>\n",
       "      <th>...</th>\n",
       "      <th>s_size_1920_818_*_s_size_720_608</th>\n",
       "      <th>s_size_352_288_squared</th>\n",
       "      <th>s_size_352_288cubic</th>\n",
       "      <th>s_size_352_288_*_s_size_720_486</th>\n",
       "      <th>s_size_352_288_*_s_size_720_608</th>\n",
       "      <th>s_size_720_486_squared</th>\n",
       "      <th>s_size_720_486cubic</th>\n",
       "      <th>s_size_720_486_*_s_size_720_608</th>\n",
       "      <th>s_size_720_608_squared</th>\n",
       "      <th>s_size_720_608cubic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.49794</td>\n",
       "      <td>-0.598653</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>-0.41539</td>\n",
       "      <td>-0.239026</td>\n",
       "      <td>-0.74125</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-1.518056</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   s_storage_size  s_duration      c_si     c_ti  \\\n",
       "0        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "1        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "2        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "3        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "4        -0.49794   -0.598653  0.653567 -0.41539   \n",
       "\n",
       "   c_scene_change_ffmpeg_ratio30  c_scene_change_ffmpeg_ratio60  \\\n",
       "0                      -0.239026                       -0.74125   \n",
       "1                      -0.239026                       -0.74125   \n",
       "2                      -0.239026                       -0.74125   \n",
       "3                      -0.239026                       -0.74125   \n",
       "4                      -0.239026                       -0.74125   \n",
       "\n",
       "   c_scene_change_ffmpeg_ratio90  c_scene_change_py_thresh30  \\\n",
       "0                      -0.512497                   -1.518056   \n",
       "1                      -0.512497                   -1.518056   \n",
       "2                      -0.512497                   -1.518056   \n",
       "3                      -0.512497                   -1.518056   \n",
       "4                      -0.512497                   -1.518056   \n",
       "\n",
       "   c_scene_change_py_thresh50  c_colorhistogram_mean_dark  ...  \\\n",
       "0                   -1.161942                   -0.214322  ...   \n",
       "1                   -1.161942                   -0.214322  ...   \n",
       "2                   -1.161942                   -0.214322  ...   \n",
       "3                   -1.161942                   -0.214322  ...   \n",
       "4                   -1.161942                   -0.214322  ...   \n",
       "\n",
       "   s_size_1920_818_*_s_size_720_608  s_size_352_288_squared  \\\n",
       "0                                 0                       0   \n",
       "1                                 0                       0   \n",
       "2                                 0                       0   \n",
       "3                                 0                       0   \n",
       "4                                 0                       0   \n",
       "\n",
       "   s_size_352_288cubic  s_size_352_288_*_s_size_720_486  \\\n",
       "0                    0                                0   \n",
       "1                    0                                0   \n",
       "2                    0                                0   \n",
       "3                    0                                0   \n",
       "4                    0                                0   \n",
       "\n",
       "   s_size_352_288_*_s_size_720_608  s_size_720_486_squared  \\\n",
       "0                                0                       0   \n",
       "1                                0                       0   \n",
       "2                                0                       0   \n",
       "3                                0                       0   \n",
       "4                                0                       0   \n",
       "\n",
       "   s_size_720_486cubic  s_size_720_486_*_s_size_720_608  \\\n",
       "0                    0                                0   \n",
       "1                    0                                0   \n",
       "2                    0                                0   \n",
       "3                    0                                0   \n",
       "4                    0                                0   \n",
       "\n",
       "   s_size_720_608_squared  s_size_720_608cubic  \n",
       "0                       0                    0  \n",
       "1                       0                    0  \n",
       "2                       0                    0  \n",
       "3                       0                    0  \n",
       "4                       0                    0  \n",
       "\n",
       "[5 rows x 850 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data augmentation\n",
    "\n",
    "# from [x, y, z] we want to obtain \n",
    "# [x, y, z, x**2, y**2, z**2, x*y, x*z, y*z]\n",
    "# aka all the degree 2 polinomyals \n",
    "\n",
    "# only consider numerical columns, exluce categorical\n",
    "\n",
    "exclude_categorical = get_numerical_columns(preprocessed_data)\n",
    "\n",
    "theoretical_final_number_of_columns = len(data.columns) + \\\n",
    "+ len(exclude_categorical) + \\\n",
    "((len(exclude_categorical)) * (len(exclude_categorical) - 1)) / 2\n",
    "\n",
    "baseForAugmentation = data[exclude_categorical]\n",
    "\n",
    "columnsCopy = baseForAugmentation.columns.tolist()\n",
    "\n",
    "print(\"original oclumns {0}\".format(len(baseForAugmentation.columns)))\n",
    "\n",
    "for c in baseForAugmentation.columns:\n",
    "\n",
    "    data[c + \"_squared\"] = data[c] ** 2\n",
    "    data[c + \"cubic\"] = data[c] ** 3\n",
    "    \n",
    "    columnsCopy.remove(c)\n",
    "    for cc in columnsCopy:\n",
    "        t = data[c] * data[cc]\n",
    "        data[c + \"_*_\" + cc ] = data[c] * data[cc]\n",
    "\n",
    "# assert(int(theoretical_final_number_of_columns) == len(data.columns.tolist()))\n",
    "\n",
    "print(\"After augmenting the data\")\n",
    "print_data_summary(data)\n",
    "augmented_data = deepcopy(data)\n",
    "augmented_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying PCA to the data\n",
      "There are 12133 datapoints\n",
      "There are 6 columns\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "\n",
    "# for PCA to work well data needs to be scaled (mean is zero)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(augmented_data.drop(['t_average_vmaf'], axis=1))\n",
    "scaledData = scaler.transform(augmented_data.drop(['t_average_vmaf'], axis=1))\n",
    "\n",
    "# reducing the data to an arbitrary number of dimensions\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(scaledData)\n",
    "pca_data = pca.transform(scaledData)\n",
    "\n",
    "# Principal components are new variables that are constructed \n",
    "# as linear combinations or mixtures of the initial variables.\n",
    "# We don't have the original headers names\n",
    "\n",
    "pca_data = pd.DataFrame(data=pca_data)\n",
    "\n",
    "if not pca_data.index.equals(augmented_data.index):\n",
    "    pca_data.index = augmented_data.index\n",
    "    \n",
    "# adding the y back\n",
    "# it was removed because otherwise it would have been lost in the pca process\n",
    "pca_data['t_average_vmaf'] = augmented_data['t_average_vmaf']\n",
    "\n",
    "print(\"After applying PCA to the data\")\n",
    "print_data_summary(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitthe 3 datasets (preprocessed, augmented, PCAed)\n",
    "# Note the the same splitting is applied across all 3 \n",
    "\n",
    "test_mask = np.random.rand(len(data), 1) > 0.60\n",
    "\n",
    "x_train_pre, y_train_pre, x_test_pre, y_test_pre = split_train_test(preprocessed_data, test_mask)\n",
    "x_train_aug, y_train_aug, x_test_aug, y_test_aug = split_train_test(augmented_data, test_mask)\n",
    "x_train_pca, y_train_pca, x_test_pca, y_test_pca = split_train_test(pca_data, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2373abfc45e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# given nn parameters, create a nn with those parameters\n",
    "# nn1 is for single layer\n",
    "# nn2 is for 2 layers\n",
    "def make_nn1_closure(firstLayerNodes, dropOutPercentage, optimiser, activation, input_dim):\n",
    "    def make_nn1():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(firstLayerNodes, activation=activation, input_dim=input_dim))\n",
    "        model.add(Dropout(dropOutPercentage))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        # plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "        return model\n",
    "    return make_nn1\n",
    "\n",
    "def make_nn2_closure(firstLayerNodes, secondLayerNodes, dropOutPercentage, optimiser, activation, input_dim):\n",
    "    def make_nn2():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(firstLayerNodes, activation=activation, input_dim=input_dim))\n",
    "        model.add(Dropout(dropOutPercentage))\n",
    "        model.add(Dense(secondLayerNodes, activation=activation))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        # plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "        return model\n",
    "    return make_nn2\n",
    "\n",
    "# Class that encapsulates all the different parameters we search for\n",
    "class Params:\n",
    "        \n",
    "        def __init__(self, first_nodes, second_nodes, epochs, kfold, opti, acti, drop, batch_size):\n",
    "            self.first_nodes = first_nodes\n",
    "            self.second_nodes = second_nodes\n",
    "            self.epochs = epochs\n",
    "            self.kfold = kfold\n",
    "            self.optimizer = opti\n",
    "            self.activation = acti\n",
    "            self.drop = drop\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        def __str__(self):\n",
    "            attrs = vars(self)\n",
    "            return ', '.join(\"%s: %s\" % item for item in attrs.items())\n",
    "        \n",
    "# Class that represent the search space (ie the grid) for the parameters\n",
    "class Grid_parameters:\n",
    "    nodeValues = list(range(5, 50))\n",
    "    batchValues = [2, 4, 8, 16, 32]\n",
    "    epochesValues = [2]#list(range(35, 75, 10))\n",
    "    kFoldSplitValues = [10, 5, 12]\n",
    "    optimizers = [\"RMSprop\", \"adam\", \"Adadelta\", \"Nadam\"]\n",
    "    activations=[\"softmax\", \"relu\", \"tanh\"]\n",
    "    dropOuts = list(np.arange(0.0, 0.8, 0.05))\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_random_params(self):\n",
    "        return Params(random.choice(self.nodeValues), \n",
    "                      random.choice(self.nodeValues),\n",
    "                      random.choice(self.epochesValues),\n",
    "                      random.choice(self.kFoldSplitValues),\n",
    "                      random.choice(self.optimizers),\n",
    "                      random.choice(self.activations),\n",
    "                      random.choice(self.dropOuts),\n",
    "                      random.choice(self.batchValues)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment for intermediary results\n",
    "# PICKLE READ FILE\n",
    "\n",
    "#infile = open('results_pre','rb')\n",
    "#new_dict = pickle.load(infile)\n",
    "#print(new_dict[0][0])\n",
    "#infile.close()\n",
    "\n",
    "grid = Grid_parameters()\n",
    "\n",
    "results_pre = []\n",
    "results_aug = []\n",
    "results_pca = []\n",
    "\n",
    "for i in range(0, 50):\n",
    "    # for 50 times get a random nn topology from the grid and get cross validaiton results for\n",
    "    # normal, augmented and PCAed data\n",
    "    params = grid.get_random_params()\n",
    "    print(params)\n",
    "    \n",
    "    estimators_1 = []\n",
    "    estimators_2 = []\n",
    "    estimators_1.append(('standardize', StandardScaler()))\n",
    "    estimators_2.append(('standardize', StandardScaler()))\n",
    "    \n",
    "    model_1 = make_nn1_closure(params.first_nodes, params.drop, params.optimizer, params.activation, x_train_pre.shape[1])\n",
    "    model_2 = make_nn2_closure(params.first_nodes, params.second_nodes, params.drop, params.optimizer, params.activation, x_train_pre.shape[1])\n",
    "    \n",
    "    estimators_1.append(('mlp', KerasRegressor(build_fn=model_1, \n",
    "                                             epochs=params.epochs, \n",
    "                                             batch_size=params.batch_size, \n",
    "                                             verbose=1)))\n",
    "    estimators_2.append(('mlp', KerasRegressor(build_fn=model_2, \n",
    "                                             epochs=params.epochs, \n",
    "                                             batch_size=params.batch_size, \n",
    "                                             verbose=1)))\n",
    "    \n",
    "    pipeline_1 = Pipeline(estimators_1)\n",
    "    pipeline_1 = Pipeline(estimators_2)\n",
    "    \n",
    "    kfold = KFold(n_splits=params.kfold)\n",
    "    \n",
    "    result_pre_1 = cross_val_score(pipeline_1, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    result_pre_2 = cross_val_score(pipeline_2, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    #result_aug = cross_val_score(pipeline, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    #result_pca = cross_val_score(pipeline, x_train_pre.to_numpy(), y_train_pre.to_numpy(), cv=kfold)\n",
    "    \n",
    "    print(\"Standardized: mean is %.2f, std is %.2f\" % (result_pre.mean(), result_pre.std()))\n",
    "    #print(\"Standardized: mean is %.2f, std is %.2f\" % (result_aug.mean(), result_aug.std()))\n",
    "    #print(\"Standardized: mean is %.2f, std is %.2f\" % (result_pca.mean(), result_pca.std()))\n",
    "    \n",
    "    results_pre.append((params, result_pre))\n",
    "    #results_aug.append((params, result_aug))\n",
    "    #results_pca.append((params, result_pca))\n",
    "    \n",
    "    # Save results to file\n",
    "    \n",
    "    with open('results_pre', 'wb') as save:\n",
    "        pickle.dump(results_pre, save)\n",
    "    \n",
    "    \n",
    "    \n",
    "# below I copied pased the best results \n",
    "\n",
    "# No Dropout\n",
    "# Standardized: mean is -46.98, std is 18.11\n",
    "            # running model with 29 firstLayerNodes, 8 secondLayerNodes,           \n",
    "            # 40 epochs, 10 batchSize, 10 k_fold_split\n",
    "\n",
    "# Standardized: mean is -48.66, std is 18.17\n",
    "            # running model with 29 firstLayerNodes, 44 secondLayerNodes,           \n",
    "            # 40 epochs, 5 batchSize, 5 k_fold_split\n",
    "\n",
    "# Standardized: mean is -51.15, std is 14.19, \n",
    "            # running model with 6 firstLayerNodes, 50 secondLayerNodes, \n",
    "            # 30 epochs, 20 batchSize, 10 k_fold_split\n",
    "        \n",
    "# Dropout after first dense layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
